---
title: "Homework 3"
author: "mm5951"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(patchwork)
```

## Problem 1

Solution provided by teaching team.


## Problem 2
Let's investigate the accelerometer dataset for the diagnosis and follow-up of congestive heart failure (CHF). 

### Data import
I first **load, tidy, and wrangle data** using the `read_csv` and `janitor::clean_names` functions. A variable to distinguish between weekdays and weekends is created using `mutate` function and placed next to the "day" column using `relocate`. 

Next, I **address the "activity_*" variables**, which correspond to the activity counts for each minute of a 24-hour day starting at midnight. The dataframe is transposed using the `pivot_longer` function, by which all activity observations are classified under "minute" according to the minute (1, 1440) of each of the five days to which they correspond, and its associated value falls under "activity_counts".

Finally, the resulting variables are coerced into number and factor types as adequate using the `as.numeric` and `factor` functions inside a `mutate`. 

```{r}
accel_df = read_csv("data_problem2/accel_data.csv") %>%
  janitor::clean_names() %>% 
  mutate(
    weekend_or_weekday = if_else((day == "Saturday" | day == "Sunday"),"Weekend","Weekday")) %>% 
  relocate(week,day_id, day,weekend_or_weekday) %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_counts",
    names_prefix = "activity_"
   ) %>% 
  mutate(
  minute = as.numeric(minute),
  day = factor(day, level = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")),
  weekend_or_weekday = factor(weekend_or_weekday, level = c("Weekday", "Weekend"))
  )
```

The `str` function reveals the nature of data of each of the variables (that is a verification).

```{r}
str(accel_df)
```

### Dataset description
The accel_df dataset contains five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

The "accel_df" dataset contains:
* `r nrow(accel_df)` observations of activity (accelerometer data), and 
* `r ncol(accel_df)` variables, which are `r names(accel_df)` that describe the information of data. 

Notably, `r names(accel_df)[5]` represents the number of minute of a 24-hour day starting at midnight, which range from `r range(pull(accel_df, minute))` minutes. Additionally, `r names(accel_df)[6]` represent the activity count of the subject of study collected by accelerometer, ranging from `r range(pull(accel_df, activity_counts))`.


### Total Daily Activity 
Next, I **aggregate across minutes** to create a daily total activity variable using the `group by` function. In order to best understand the data, it is summarized into a table using the `summarize` and then `knitr::kable` functions.

```{r}
total_act_df = accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity_counts)) 
knitr::kable(total_act_df)
```

Next, I **investigate apparent trends** using the generated table out of "total_act_df", as well as some exploratory analyses graphs. To generate them, the `ggplot` function is used, placing the "day_id" in the X axis and "total_activity" in the Y axis. A scatterplot is generated with the `geom_point` argument, and points are united into a continous trend line using `geom_line`. The resulting graph characteristics are adjusted using `scale_x_continous` and `theme` to ensure readability.

```{r}
ggplot(total_act_df,aes(x = day_id, y = total_activity)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Daily Total Activity (for the study period of 5 weeks)",
    x = "Day ID",
    y = "Total Activity counts",
    caption = "Exploratory Analysis from the accel_data.csv Dataset"
  ) +
  scale_x_continuous(breaks = seq(0,35,by = 5)) +
  theme(plot.title = element_text(hjust = 0.5))
```

After exploratory analysis, no apparent trend of daily total activity becomes apparent, rather I conclude it has changed substantially daily over the five weeks of study period. Therefore, aggregating by total daily activity does not seem to be of extreme value when analysing the "accel_df" dataset. 

### 24-hour Daily Activity
In order to investigate the dataset differently, I **investigate the accelerometer-recorded activity over the course of the day**. To do so, a different graph is generated using `ggplot`: this time, I place the minutes in the X axis, and the activity counts in the Y axis. As per homework instructions, each day of the week is given a different color. The graph details are adjusted similarly as above.

```{r fig.width = 10, fig.height = 9}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day)) +
  geom_point() +
  geom_line(alpha = .3) +
  scale_x_continuous(breaks = seq(0,1440,by = 60),
                     label = seq(0,24,by = 1),
                     limits = c(0,1440)) +
  labs(
    title = "24-hour activity time courses for each day of the week",
    x = "Hours",
    y = "Activity counts",
    caption = "Exploratory Analysis from the accel_data.csv Dataset"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

Some patterns become apparent when looking into the resulting visualization. First, the studied individual appears to be active between 6 AM and 22 AM daily. Some daily patterns are to be noted: for instance, Fridays at 9 AM there's an activity peak, and so there is Wednesdays at 8 PM. 

## Problem 3
This problem uses the NY NOAA data to conduct exploratory analyses. I will first **load the data from the p8105.datasets package** using the `library` function.

```{r}
library(p8105.datasets)
data("ny_noaa")
```

### NOAA dataset content description
To have a **general overview of the ny_noaa dataset**, I use the `str` and `skimr::skim` functions to obtain a summary of the internal data structure of this dataframe and the key metrics of its content.

```{r}
str(ny_noaa)
skimr::skim(ny_noaa)
```

In summary, the "ny_noaa" dataset is characterized by:
* contains data from 1981-01-01	to 2010-12-31	time period.
* `r ncol(ny_noaa)` variables, which include: `r names(ny_noaa)`.
* `r nrow(ny_noaa)` weather observations.
* there's substantial missing data, notably n=1,134,358 observations of maximum temperature, n=1,134,420 of minimum temperature, n=145,838  of precipitation, n=381,221 of snow and n=591,786 of snowd. 


### NOAA data wrangling
Next, I do some **data cleaning and wrangling** to generate the "data_df" including:
* Set reasonable names using `janitor::clean_names` function
* Create separate variables for year, month, and day using `separate` 
* Coerce the right data type for temperature, precipitation, and snowfall observations using `mutate` in combination with `as.numeric`and similar arguments.

```{r}
noaa_df = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = '-') %>%
  mutate(month = as.numeric(month),
         year = as.numeric(year),
         month = month.abb[month],
         month = factor(month, levels = month.abb),
         tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         prcp = as.numeric(prcp))
```

In order to assess the snowfall the most commonly observed values, I generate a function `find_mode` to estimate the mode (see below), and then use it for the "snow" and "snwd" variables. In both cases, the mode is 0 (that is, most days during the year it doesn't snow).

```{r}
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

find_mode(noaa_df$snow)
find_mode(noaa_df$snwd)
```

### NOAA data visualization (I): min & max temperatures
Next, using `ggplot` I make a **two-panel plot showing the average max temperature in January and in July** in each station across years. 

```{r}

```

As per the data visualization results, 
Is there any observable / interpretable structure? Any outliers?

### NOAA data visualization (II): snowfall distribution
Next, using a similar function as above, I generate a **two-panel plot showing tmax vs tmin for the full dataset** and **the distribution of snowfall**, with values greater than 0 and less than 100 separately by year.

```{r}

```

