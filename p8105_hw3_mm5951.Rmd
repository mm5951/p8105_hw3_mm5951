---
title: "Homework 3"
author: "mm5951"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(patchwork)
```

## Problem 1

Solution provided by teaching team.


## Problem 2
Let's investigate the accelerometer dataset for the diagnosis and follow-up of congestive heart failure (CHF). 

### Data import
I first **load, tidy, and wrangle data** using the `read_csv` and `janitor::clean_names` functions. A variable to distinguish between weekdays and weekends is created using `mutate` function and placed next to the "day" column using `relocate`. 

Next, I **address the "activity_*" variables**, which correspond to the activity counts for each minute of a 24-hour day starting at midnight. The dataframe is transposed using the `pivot_longer` function, by which all activity observations are classified under "minute" according to the minute (1, 1440) of each of the five days to which they correspond, and its associated value falls under "activity_counts".

Finally, the resulting variables are coerced into number and factor types as adequate using the `as.numeric` and `factor` functions inside a `mutate`. 

```{r}
accel_df = read_csv("data_problem2/accel_data.csv") %>%
  janitor::clean_names() %>% 
  mutate(
    weekend_or_weekday = if_else((day == "Saturday" | day == "Sunday"),"Weekend","Weekday")) %>% 
  relocate(week,day_id, day,weekend_or_weekday) %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_counts",
    names_prefix = "activity_"
   ) %>% 
  mutate(
  minute = as.numeric(minute),
  day = factor(day, level = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")),
  weekend_or_weekday = factor(weekend_or_weekday, level = c("Weekday", "Weekend"))
  )
```

The `str` function reveals the nature of data of each of the variables (that is a verification).

```{r}
str(accel_df)
```

### Dataset description
The accel_df dataset contains five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

The "accel_df" dataset contains:
* `r nrow(accel_df)` observations of activity (accelerometer data), and 
* `r ncol(accel_df)` variables, which are `r names(accel_df)` that describe the information of data. 

Notably, `r names(accel_df)[5]` represents the number of minute of a 24-hour day starting at midnight, which range from `r range(pull(accel_df, minute))` minutes. Additionally, `r names(accel_df)[6]` represent the activity count of the subject of study collected by accelerometer, ranging from `r range(pull(accel_df, activity_counts))`.


### Total Daily Activity 
Next, I **aggregate across minutes** to create a daily total activity variable using the `group by` function. In order to best understand the data, it is summarized into a table using the `summarize` and then `knitr::kable` functions.

```{r}
total_act_df = accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity_counts)) 
knitr::kable(total_act_df)
```

Next, I **investigate apparent trends** using the generated table out of "total_act_df", as well as some exploratory analyses graphs. To generate them, the `ggplot` function is used, placing the "day_id" in the X axis and "total_activity" in the Y axis. A scatterplot is generated with the `geom_point` argument, and points are united into a continous trend line using `geom_line`. The resulting graph characteristics are adjusted using `scale_x_continous` and `theme` to ensure readability.

```{r}
ggplot(total_act_df,aes(x = day_id, y = total_activity)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Daily Total Activity (for the study period of 5 weeks)",
    x = "Day ID",
    y = "Total Activity counts",
    caption = "Exploratory Analysis from the accel_data.csv Dataset"
  ) +
  scale_x_continuous(breaks = seq(0,35,by = 5)) +
  theme(plot.title = element_text(hjust = 0.5))
```

After exploratory analysis, no apparent trend of daily total activity becomes apparent, rather I conclude it has changed substantially daily over the five weeks of study period. Therefore, aggregating by total daily activity does not seem to be of extreme value when analysing the "accel_df" dataset. 

### 24-hour Daily Activity
In order to investigate the dataset differently, I **investigate the accelerometer-recorded activity over the course of the day**. To do so, a different graph is generated using `ggplot`: this time, I place the minutes in the X axis, and the activity counts in the Y axis. As per homework instructions, each day of the week is given a different color. The graph details are adjusted similarly as above.

```{r fig.width = 10, fig.height = 9}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_counts, color = day)) +
  geom_point() +
  geom_line(alpha = .3) +
  scale_x_continuous(breaks = seq(0,1440,by = 60),
                     label = seq(0,24,by = 1),
                     limits = c(0,1440)) +
  labs(
    title = "24-hour activity time courses for each day of the week",
    x = "Hours",
    y = "Activity counts",
    caption = "Exploratory Analysis from the accel_data.csv Dataset"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

Some patterns become apparent when looking into the resulting visualization. First, the studied individual appears to be active between 6 AM and 22 AM daily. Some daily patterns are to be noted: for instance, Fridays at 9 AM there's an activity peak, and so there is Wednesdays at 8 PM. 

## Problem 3
This problem uses the NY NOAA data to conduct exploratory analyses. I will first **load the data from the p8105.datasets package** using the `library` function.

```{r}
library(p8105.datasets)
data("ny_noaa")
```

### Dataset content description
The "ny_noaa" dataset is characterized by:
* 

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

### Next section
Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.


